{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distillation of object detection pipeline\n",
    "\n",
    "## Goals\n",
    "* Minimize steps - align more closely with the steps in a final pipeline\n",
    "    * For now we will neglect manual labeling, just leave a space for it\n",
    "* Minimize code footprint that needs to go to docker\n",
    "* Pull as much from git or S3 directly as possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put all imports and globals here\n",
    "This way it's easy to re-run this if you want to jump somewhere into the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "\n",
    "SM_SESSION = sagemaker.Session()\n",
    "SM_ROLE = sagemaker.get_execution_role()\n",
    "\n",
    "TOPDIR = os.getcwd()\n",
    "\n",
    "# This is the code we ship to training.  Keep this light and your training\n",
    "# will start faster\n",
    "CODE_SRCDIR = f\"{TOPDIR}/train_src\"\n",
    "\n",
    "TF_MODELS_GIT=\"https://github.com/tensorflow/models.git\"\n",
    "TF_MODELS_SRCDIR=f\"{TOPDIR}/tf_models\"\n",
    "\n",
    "# These are the discrete images and annotations we pull from S3\n",
    "DATA_SRCDIR = f\"{TOPDIR}/srcdata\"\n",
    "TARBALL_SRCDIR = f\"{DATA_SRCDIR}/tarballs\"\n",
    "TARBALL_STAGING = f\"{DATA_SRCDIR}/tarball_extract_tmp\"\n",
    "ANNOTATION_SRCDIR = f\"{DATA_SRCDIR}/annotations\"\n",
    "JPEG_SRCDIR = f\"{DATA_SRCDIR}/jpeg_images\"\n",
    "\n",
    "# These are the tfrecord files which hold the above data\n",
    "TFRECORD_SRCDIR = f\"{DATA_SRCDIR}/tfrecords\"\n",
    "TRAIN_TFRECORD_SRCDIR = f\"{TFRECORD_SRCDIR}/train\"\n",
    "VALIDATE_TFRECORD_SRCDIR = f\"{TFRECORD_SRCDIR}/val\"\n",
    "TEST_TFRECORD_SRCDIR = f\"{TFRECORD_SRCDIR}/test\"\n",
    "\n",
    "# Label -> label ID used by the model\n",
    "LABEL_MAP_FILE = f\"{CODE_SRCDIR}/cfa_prod_label_map.pbtxt\"\n",
    "\n",
    "# S3 location of our source images\n",
    "S3_ALL_IMAGES = \"s3://cfa-eadatasciencesb-sagemaker/datasets/cfa_products/all_images/\"\n",
    "S3_ALL_ANNOTATIONS = \"s3://cfa-eadatasciencesb-sagemaker/datasets/cfa_products/all_annotations\"\n",
    "S3_TEST_IMAGES = \"s3://cfa-eadatasciencesb-sagemaker/datasets/cfa_products/test_images\"\n",
    "\n",
    "# Our SM jobs will use this bucket\n",
    "SM_WORKING_S3_BUCKET = \"dev-eadatasciencesb-us-east-1-sagemaker-pdamore\"\n",
    "\n",
    "# S3 locations of our inputs\n",
    "# Train tfrecords\n",
    "S3_TRAIN_TFRECORDS_URI=f\"s3://{SM_WORKING_S3_BUCKET}/datasets/cfa_products/train\"\n",
    "# Validate tfrecords\n",
    "S3_VALIDATE_TFRECORDS_URI=f\"s3://{SM_WORKING_S3_BUCKET}/datasets/cfa_products/val\"\n",
    "# Since we are transfer training, this is the path of the model we want to start with\n",
    "# This will get passed as an input channel to the training job\n",
    "S3_BASEMODEL_URI=\"s3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized\"\n",
    "\n",
    "S3_OUTPUT_URI=f\"s3://{SM_WORKING_S3_BUCKET}/outputs\"\n",
    "\n",
    "# Train, Validate, Test\n",
    "TRAINING_SPLIT_TUPLE =  (60,30,10)\n",
    "NUM_TRAIN_STEPS = '1'\n",
    "NUM_VALIDATE_STEPS = '1'\n",
    "MODEL_VERSION = \"ptd002\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in the tf models code\n",
    "Have to do this early so we can import more stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping git clone of tf models because it exists.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(TF_MODELS_SRCDIR):\n",
    "    ! git clone {TF_MODELS_GIT} {TF_MODELS_SRCDIR}\n",
    "else:\n",
    "    print(\"Skipping git clone of tf models because it exists.\")\n",
    "\n",
    "if not os.path.isdir(f\"{CODE_SRCDIR}/object_detection\"):\n",
    "    ! cp -r {TF_MODELS_SRCDIR}/research/object_detection {CODE_SRCDIR}\n",
    "    ! cp -r {TF_MODELS_SRCDIR}/research/slim {CODE_SRCDIR}\n",
    "    ! rm -rf {CODE_SRCDIR}/object_detection/test_ckpt\n",
    "    ! rm -rf {CODE_SRCDIR}/object_detection/g3doc\n",
    "    ! pushd {CODE_SRCDIR}; protoc object_detection/protos/*.proto --python_out=.; popd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now import the rest of the local code with the tf model dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(CODE_SRCDIR)\n",
    "from cfa_utils.tar_util import extract_tarball_directory\n",
    "from cfa_utils.example_utils import voc_to_tfrecord_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the filesystem environment\n",
    "Please keep this reasonably idempotent\n",
    "### Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is .gitignore'd so it's reasonable to not exist\n",
    "os.makedirs(DATA_SRCDIR, exist_ok=True)\n",
    "os.makedirs(f\"{TARBALL_SRCDIR}\", exist_ok=True)\n",
    "os.makedirs(f\"{TARBALL_STAGING}\", exist_ok=True)\n",
    "os.makedirs(f\"{JPEG_SRCDIR}\", exist_ok=True)\n",
    "os.makedirs(f\"{ANNOTATION_SRCDIR}\", exist_ok=True)\n",
    "os.makedirs(f\"{TRAIN_TFRECORD_SRCDIR}\", exist_ok=True)\n",
    "os.makedirs(f\"{VALIDATE_TFRECORD_SRCDIR}\", exist_ok=True)\n",
    "os.makedirs(f\"{TEST_TFRECORD_SRCDIR}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull in raw images and annotations\n",
    "Taken from UnderstandingImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping jpeg copy, files aready exist\n",
      "Skipping annotation copy, files aready exist\n"
     ]
    }
   ],
   "source": [
    "if len(os.listdir(f\"{JPEG_SRCDIR}\")) == 0:\n",
    "    ! rm -f {TARBALL_SRCDIR}/*\n",
    "    ! aws s3 cp {S3_ALL_IMAGES} {TARBALL_SRCDIR} --recursive --quiet\n",
    "    jpg_ext = '.jpg'\n",
    "    r = extract_tarball_directory(TARBALL_SRCDIR, TARBALL_STAGING, jpg_ext, JPEG_SRCDIR)\n",
    "    print(f\"jpeg file count: {r}\")\n",
    "else:\n",
    "    print(\"Skipping jpeg copy, files aready exist\")\n",
    "\n",
    "if len(os.listdir(f\"{ANNOTATION_SRCDIR}\")) == 0:\n",
    "    ! rm -f {TARBALL_SRCDIR}/*\n",
    "    ! aws s3 cp {S3_ALL_ANNOTATIONS} {TARBALL_SRCDIR} --recursive --quiet\n",
    "    xml_ext = '.xml'\n",
    "    r = extract_tarball_directory(TARBALL_SRCDIR, TARBALL_STAGING, xml_ext, ANNOTATION_SRCDIR)\n",
    "    print(f\"annotation file count: {r}\")\n",
    "    # This is from Make_TFRecords.  Apparently the labeling was not consistent and this normalizes it\n",
    "    ! sed -i 's/smHotDrink/smallHotDrink/g' {ANNOTATION_SRCDIR}/*.xml\n",
    "    ! sed -i 's/medColdDrink/mediumColdDrink/g' {ANNOTATION_SRCDIR}/*.xml\n",
    "    ! sed -i 's/smallSauce/cfaSauce/g' {ANNOTATION_SRCDIR}/*.xml\n",
    "else:\n",
    "    print(\"Skipping annotation copy, files aready exist\")\n",
    "\n",
    "! rm -f {TARBALL_SRCDIR}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tfrecord files\n",
    "From Make_TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping voc_to_tfrecord, already have tfrecord files\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(f\"{TRAIN_TFRECORD_SRCDIR}/train.tfrecord\"):\n",
    "    voc_to_tfrecord_file(JPEG_SRCDIR,\n",
    "                        ANNOTATION_SRCDIR,\n",
    "                        LABEL_MAP_FILE,\n",
    "                        TFRECORD_SRCDIR,\n",
    "                        TRAINING_SPLIT_TUPLE)\n",
    "else:\n",
    "    print(\"Skipping voc_to_tfrecord, already have tfrecord files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to train\n",
    "\n",
    "Taken from TrainModel_Step3_TrainingJob\n",
    "\n",
    "Now we have the data we need to train the model.  We are going to go right to training in a SM Training job as that is our desired end state.\n",
    "\n",
    "### First put the tfrecord files into S3\n",
    "\n",
    "**Note** I did not have a great way to make this idempotent, so it's not!  Uncomment and run if you need to put stuff in your S3 bucket!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please uncomment me if you need to upload the tfrecords to S3!\n"
     ]
    }
   ],
   "source": [
    "print(\"Please uncomment me if you need to upload the tfrecords to S3!\")\n",
    "#print(S3_TRAIN_TFRECORDS_URI)\n",
    "#print(S3_VALIDATE_TFRECORDS_URI)\n",
    "#!aws s3 cp {TRAIN_TFRECORD_SRCDIR}/*.tfrecord {S3_TRAIN_TFRECORDS_URI}/\n",
    "#!aws s3 cp {VALIDATE_TFRECORD_SRCDIR}/*.tfrecord {S3_VALIDATE_TFRECORDS_URI}/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fine-tune checkpoint as a SageMaker input channel\n",
    "We add a new input channel (basemodel, maybe not a great name but it made sense to me) and simply point to where our model lives in S3.  Then it gets pulled at runtime.\n",
    "\n",
    "I have another variation of this in ``train.py`` which can also pull it in the script from S3.\n",
    "\n",
    "We need to figure out which is better for tracability, here or in the train script, because we will need to track this as part of any version metadata we create!\n",
    "\n",
    "More detail here https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://dev-eadatasciencesb-us-east-1-sagemaker-pdamore/datasets/cfa_products/train', 'val': 's3://dev-eadatasciencesb-us-east-1-sagemaker-pdamore/datasets/cfa_products/val', 'basemodel': 's3://cfa-eadatasciencesb-sagemaker/trained-models/tensorflow_mobilenet/20180718_coco14_mobilenet_v1_ssd300_quantized'}\n"
     ]
    }
   ],
   "source": [
    "model_dir = '/opt/ml/model'\n",
    "# Keep this relatively cheap - $1.26/hr.  This demo also slashes the step count.  The model won't be great\n",
    "# bit it works passably.\n",
    "train_instance_type = 'ml.p2.xlarge'\n",
    "#train_instance_type = 'local'\n",
    "# Path is relative to the src_dir\n",
    "hyperparameters = {'pipeline_config_path' : 'sagemaker_mobilenet_v1_ssd_retrain.config',\n",
    "                   'num_train_steps' : NUM_TRAIN_STEPS,\n",
    "                   'num_eval_steps' : NUM_VALIDATE_STEPS\n",
    "                  }\n",
    "inputs = {'train': S3_TRAIN_TFRECORDS_URI, 'val': S3_VALIDATE_TFRECORDS_URI, 'basemodel': S3_BASEMODEL_URI}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting output path\n",
    "We set a base path in S3 for the output files, so they don't hit the default bucket.  When you do this, the source upload directory is also based on output_path, unless you specify a code_location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                       source_dir='train_src',\n",
    "                       model_dir=model_dir,\n",
    "                       output_path=S3_OUTPUT_URI,\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=SM_ROLE,\n",
    "                       base_job_name='cfa-products-mobilenet-v1-ssd',\n",
    "                       framework_version='1.14',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-15 03:18:48 Starting - Starting the training job...\n",
      "2019-10-15 03:18:49 Starting - Launching requested ML instances......\n",
      "2019-10-15 03:19:50 Starting - Preparing the instances for training......\n",
      "2019-10-15 03:21:06 Downloading - Downloading input data...\n",
      "2019-10-15 03:21:49 Training - Downloading the training image.....\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[31m--> installing: cython\u001b[0m\n",
      "\u001b[31mCollecting cython\u001b[0m\n",
      "\u001b[31m  Downloading https://files.pythonhosted.org/packages/45/f2/a7101b3457561e57f5abcd6f5ac13190054fecd7370f58f36fe2d6574742/Cython-0.29.13-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\u001b[0m\n",
      "\u001b[31mInstalling collected packages: cython\u001b[0m\n",
      "\u001b[31mSuccessfully installed cython-0.29.13\u001b[0m\n",
      "\u001b[31mWARNING: You are using pip version 19.2.1, however version 19.3 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m--> installing: pycocotools\u001b[0m\n",
      "\u001b[31mCollecting pycocotools\n",
      "  Downloading https://files.pythonhosted.org/packages/96/84/9a07b1095fd8555ba3f3d519517c8743c2554a245f9476e5e39869f948d2/pycocotools-2.0.0.tar.gz (1.5MB)\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py): started\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2019-10-15 03:22:20 Training - Training image download completed. Training in progress.\u001b[31m  Building wheel for pycocotools (setup.py): finished with status 'done'\n",
      "  Created wheel for pycocotools: filename=pycocotools-2.0.0-cp36-cp36m-linux_x86_64.whl size=288491 sha256=c77dd8ab6b5ed87a61e98a08bf068b0400869d1a9dd91ce94159a87f4adf1706\n",
      "  Stored in directory: /root/.cache/pip/wheels/dc/e6/36/0e1ae88c868eb42d3f92181b1c9bbd0b217a7ec3da6bd62e55\u001b[0m\n",
      "\u001b[31mSuccessfully built pycocotools\u001b[0m\n",
      "\u001b[31mInstalling collected packages: pycocotools\u001b[0m\n",
      "\u001b[31mSuccessfully installed pycocotools-2.0.0\u001b[0m\n",
      "\u001b[31mWARNING: You are using pip version 19.2.1, however version 19.3 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m--> installing: matplotlib\u001b[0m\n",
      "\u001b[31mCollecting matplotlib\n",
      "  Downloading https://files.pythonhosted.org/packages/57/4f/dd381ecf6c6ab9bcdaa8ea912e866dedc6e696756156d8ecc087e20817e2/matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\u001b[0m\n",
      "\u001b[31mCollecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/site-packages (from matplotlib) (2.8.0)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/site-packages (from matplotlib) (1.17.0)\u001b[0m\n",
      "\u001b[31mCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/11/fa/0160cd525c62d7abd076a070ff02b2b94de589f1a9789774f17d7c54058e/pyparsing-2.4.2-py2.py3-none-any.whl (65kB)\u001b[0m\n",
      "\u001b[31mCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/a1/5742b56282449b1c0968197f63eae486eca2c35dcd334bab75ad524e0de1/kiwisolver-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (90kB)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\u001b[0m\n",
      "\u001b[31mRequirement already satisfied: setuptools in /usr/local/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.0.1)\u001b[0m\n",
      "\u001b[31mInstalling collected packages: cycler, pyparsing, kiwisolver, matplotlib\u001b[0m\n",
      "\u001b[31mSuccessfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.1.1 pyparsing-2.4.2\u001b[0m\n",
      "\u001b[31mWARNING: You are using pip version 19.2.1, however version 19.3 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mWARNING: Logging before flag parsing goes to stderr.\u001b[0m\n",
      "\u001b[31mW1015 03:22:42.582799 140612843366144 lazy_loader.py:50] \u001b[0m\n",
      "\u001b[31mThe TensorFlow contrib module will not be included in TensorFlow 2.0.\u001b[0m\n",
      "\u001b[31mFor more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\u001b[0m\n",
      "\u001b[31mIf you depend on functionality not listed there, please file an issue.\n",
      "\u001b[0m\n",
      "\u001b[31mW1015 03:22:42.607003 140612843366144 deprecation_wrapper.py:119] From slim/nets/inception_resnet_v2.py:373: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\u001b[0m\n",
      "\u001b[31mW1015 03:22:42.619585 140612843366144 deprecation_wrapper.py:119] From slim/nets/mobilenet/mobilenet.py:397: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\u001b[0m\n",
      "\u001b[31mW1015 03:22:42.625257 140612843366144 deprecation_wrapper.py:119] From train.py:188: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\u001b[0m\n",
      "\u001b[31m*** train.py/main()\u001b[0m\n",
      "\u001b[31m*** FLAGS ***\u001b[0m\n",
      "\u001b[31mpipeline_config_path: sagemaker_mobilenet_v1_ssd_retrain.config\u001b[0m\n",
      "\u001b[31mconfig exists: True\u001b[0m\n",
      "\u001b[31mfile: cfa_prod_label_map.pbtxt\u001b[0m\n",
      "\u001b[31mfile: __init__.py\u001b[0m\n",
      "\u001b[31mfile: slim\u001b[0m\n",
      "\u001b[31mfile: train.py.orig\u001b[0m\n",
      "\u001b[31mfile: object_detection\u001b[0m\n",
      "\u001b[31mfile: train.py\u001b[0m\n",
      "\u001b[31mfile: sagemaker_mobilenet_v1_ssd_retrain.config\u001b[0m\n",
      "\u001b[31mfile: cfa_utils\u001b[0m\n",
      "\u001b[31mmodel_dir: /opt/ml/model\u001b[0m\n",
      "\u001b[31mtrain: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[31mval: /opt/ml/input/data/val\u001b[0m\n",
      "\u001b[31msample_1_of_n_eval_examples: 1\u001b[0m\n",
      "\u001b[31mhparams_overrides: None\u001b[0m\n",
      "\u001b[31mcheckpoint_dir: None\u001b[0m\n",
      "\u001b[31mW1015 03:22:42.626331 140612843366144 deprecation_wrapper.py:119] From /opt/ml/code/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\u001b[0m\n",
      "\u001b[31mchecking inputs for: train_input_config\u001b[0m\n",
      "\u001b[31mpath: True /opt/ml/input/data/train/train.tfrecord\u001b[0m\n",
      "\u001b[31mchecking inputs for: eval_input_config\u001b[0m\n",
      "\u001b[31mpath: True /opt/ml/input/data/val/val.tfrecord\u001b[0m\n",
      "\u001b[31mchecking for basemodel existance:\u001b[0m\n",
      "\u001b[31mpath: True /opt/ml/input/data/basemodel/model.ckpt.meta\n",
      " - - - - - - - - -\u001b[0m\n",
      "\u001b[31mW1015 03:22:42.633073 140612843366144 deprecation_wrapper.py:119] From train.py:137: The name tf.enable_eager_execution is deprecated. Please use tf.compat.v1.enable_eager_execution instead.\n",
      "\u001b[0m\n",
      "\u001b[31mW1015 03:22:42.633346 140612843366144 deprecation_wrapper.py:119] From train.py:138: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\u001b[0m\n",
      "\u001b[31mW1015 03:22:42.633602 140612843366144 deprecation_wrapper.py:119] From train.py:139: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[0m\n",
      "\u001b[31mW1015 03:22:42.633793 140612843366144 deprecation_wrapper.py:119] From train.py:139: The name tf.logging.ERROR is deprecated. Please use tf.compat.v1.logging.ERROR instead.\n",
      "\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.166654 140612843366144 variables_helper.py:154] Variable [BoxPredictor_0/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[273]], model variable shape: [[36]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.166931 140612843366144 variables_helper.py:154] Variable [BoxPredictor_0/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 273]], model variable shape: [[1, 1, 512, 36]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.167127 140612843366144 variables_helper.py:154] Variable [BoxPredictor_1/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[72]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.167246 140612843366144 variables_helper.py:154] Variable [BoxPredictor_1/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 1024, 546]], model variable shape: [[1, 1, 1024, 72]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.167438 140612843366144 variables_helper.py:154] Variable [BoxPredictor_2/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[72]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.167557 140612843366144 variables_helper.py:154] Variable [BoxPredictor_2/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 546]], model variable shape: [[1, 1, 512, 72]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.167717 140612843366144 variables_helper.py:154] Variable [BoxPredictor_3/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[72]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.167836 140612843366144 variables_helper.py:154] Variable [BoxPredictor_3/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 72]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.167998 140612843366144 variables_helper.py:154] Variable [BoxPredictor_4/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[72]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.168113 140612843366144 variables_helper.py:154] Variable [BoxPredictor_4/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 72]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.168287 140612843366144 variables_helper.py:154] Variable [BoxPredictor_5/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[72]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.168406 140612843366144 variables_helper.py:154] Variable [BoxPredictor_5/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 546]], model variable shape: [[1, 1, 128, 72]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mW1015 03:22:48.170678 140612843366144 variables_helper.py:157] Variable [global_step] is not available in checkpoint\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m2019-10-15 03:23:21.467967: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=4.07s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.85s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\u001b[0m\n",
      "\n",
      "2019-10-15 03:25:13 Uploading - Uploading generated training model\n",
      "2019-10-15 03:25:28 Completed - Training job completed\n",
      "Training seconds: 262\n",
      "Billable seconds: 262\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and endpoint names, based on a version at top of file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: model-mobilenet-v1-ssd-cfa-products-ptd002\n",
      "endpoint: ep-mobilenet-v1-ssd-cfa-products-ptd002\n"
     ]
    }
   ],
   "source": [
    "# this is where you'll be glad you used a new version number\n",
    "\n",
    "model_name = 'model-mobilenet-v1-ssd-cfa-products-{}'.format(MODEL_VERSION)\n",
    "endpoint_name = 'ep-mobilenet-v1-ssd-cfa-products-{}'.format(MODEL_VERSION)\n",
    "print (\"model:\", model_name)\n",
    "print (\"endpoint:\", endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Model object and deploy an endpoint.\n",
    "\n",
    "I had originally thought that we could call estimator.create_model() to get the Model created in SM, but this only creates a programmatic Model object.  You still have to then call deploy() on the model.  There is also an issue passing ``name=`` to the create_model method, because it classes with something in the Estimator code doing the same thing and it fails.  The Estimator actually creates the model, then directly sets the name attribute before deploy().\n",
    "\n",
    "It seems we are down to two methods to do this:\n",
    "\n",
    "1. Call model._create_sagemaker_model(), but this is a _ method and feels wrong\n",
    "2. Make sure we are highly opinionated about our output artifact names so we can easily find them in S3 at deploy time, and create the Model at that point.\n",
    "2a. We could choose at build time to immediately create an endpoint for testing, which obviates this whole issue.\n",
    "3. If it's really important, maybe just use boto?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1,\n",
    "                             instance_type='ml.p2.xlarge',\n",
    "                             model_name=model_name,\n",
    "                             endpoint_name=endpoint_name\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same stuff as before to get a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This actually came from cfa_utils and I should be using it there!\n",
    "def bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "IMAGE_DIR = os.getcwd()\n",
    "image_filename = \"srcdata/jpeg_images/20190710_variety_1562781017.jpg\"\n",
    "image_file_path = os.path.join(IMAGE_DIR, image_filename)\n",
    "pil_image = Image.open(image_file_path)\n",
    "\n",
    "# Trying to use tf.io where I can.  This notebook is not eaguer though, so I cheat a bit later\n",
    "f = tf.io.read_file(image_file_path)\n",
    "\n",
    "feature = {}\n",
    "#features['image/encoded'] = tf.io.FixedLenFeature((), tf.string, default_value='')\n",
    "# This structure mimics what Jay defines in cfa_utils.  Need to try just passing\n",
    "# in the whole thing with only image/encoded filled in so we can share the code\n",
    "# Cheat with the tf.Session because I forgot to set eager mode earlier\n",
    "feature['image/encoded'] = bytes_feature(tf.Session().run(f))\n",
    "features = tf.train.Features(feature=feature)\n",
    "ex = tf.train.Example(features=features)\n",
    "ex_str = ex.SerializePartialToString()\n",
    "\n",
    "\n",
    "# plt.imshow(pil_image)\n",
    "# plt.show()\n",
    "d = {'signature_name': 'serving_default', 'instances': [{'b64': base64.standard_b64encode(ex_str).decode('ascii')}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = predictor.predict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure we got a result\n",
    "TODO: pull in Jay's new code to visualize the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
